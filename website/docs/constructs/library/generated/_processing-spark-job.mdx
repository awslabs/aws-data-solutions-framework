[//]: # (This file is generated, do not modify directly, update the README.md in framework/src/processing)
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

A construct to create a Spark job that is orchestrated through AWS Step Functions state machine. The state machine can submit a job with either [Amazon EMR on EKS](https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/getting-started.html) or [Amazon EMR Serverless](https://docs.aws.amazon.com/emr/latest/EMR-Serverless-UserGuide/getting-started.html).

## Overview

The construct creates an AWS Step Functions state machine that is used to submit a Spark job and orchestrate the lifecycle of the job. The construct leverages the [AWS SDK service integrations](https://docs.aws.amazon.com/step-functions/latest/dg/supported-services-awssdk.html) to submit the jobs. The state machine can take a cron expression to trigger the job at a given interval. The schema below shows the state machine:


![Spark Job State Machine](../../../../static/img/adsf-spark-job-statemachine.svg)

## Usage

### Define an EMR Serverless Spark Job

The example stack below shows how to use `EmrServerlessSparkJob` construct. The stack also contains a `SparkEmrServerlessRuntime` to show how to create an EMR Serverless Application and pass it as an argument to the `Spark job` and use it as a runtime for the job.

<Tabs>
  <TabItem value="typescript" label="TypeScript" default>

  ```typescript
  class ExampleSparkJobEmrServerlessStack extends cdk.Stack {
    constructor(scope: Construct, id: string) {
        super(scope, id);
        const runtime = new dsf.processing.SparkEmrServerlessRuntime(this, 'SparkRuntime', {
            name: 'mySparkRuntime',
        });

        const s3ReadPolicy = new PolicyDocument({
            statements: [
                PolicyStatement.fromJson({
                    actions: ['s3:GetObject'],
                    resources: ['arn:aws:s3:::bucket_name', 'arn:aws:s3:::bucket_name/*'],
                }),
            ],
        });

        const executionRole = dsf.processing.SparkEmrServerlessRuntime.createExecutionRole(this, 'EmrServerlessExecutionRole', s3ReadPolicy);

        const nightJob = new dsf.processing.SparkEmrServerlessJob(this, 'SparkNightlyJob', {
            applicationId: runtime.applicationId,
            name: 'nightly_job',
            executionRoleArn: executionRole.roleArn,
            executionTimeoutMinutes: 30,
            s3LogUri: 's3://emr-job-logs-EXAMPLE/logs',
            sparkSubmitEntryPoint: 'local:///usr/lib/spark/examples/src/main/python/pi.py',
            sparkSubmitParameters: '--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4',
        });

        new CfnOutput(this, 'job-state-machine', {
            value: nightJob.stateMachine!.stateMachineArn,
        });
    }
}
  ```
  
  ```mdx-code-block
  
  </TabItem>
  <TabItem value="python" label="Python">

  ```python
  class ExampleSparkJobEmrServerlessStack(cdk.Stack):
    def __init__(self, scope, id):
        super().__init__(scope, id)
        runtime = dsf.processing.SparkEmrServerlessRuntime(self, "SparkRuntime",
            name="mySparkRuntime"
        )

        s3_read_policy = PolicyDocument(
            statements=[
                PolicyStatement.from_json({
                    "actions": ["s3:GetObject"],
                    "resources": ["arn:aws:s3:::bucket_name", "arn:aws:s3:::bucket_name/*"]
                })
            ]
        )

        execution_role = dsf.processing.SparkEmrServerlessRuntime.create_execution_role(self, "EmrServerlessExecutionRole", s3_read_policy)

        night_job = dsf.processing.SparkEmrServerlessJob(self, "SparkNightlyJob",
            application_id=runtime.application_id,
            name="nightly_job",
            execution_role_arn=execution_role.role_arn,
            execution_timeout_minutes=30,
            s3_log_uri="s3://emr-job-logs-EXAMPLE/logs",
            spark_submit_entry_point="local:///usr/lib/spark/examples/src/main/python/pi.py",
            spark_submit_parameters="--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4"
        )

        CfnOutput(self, "job-state-machine",
            value=night_job.state_machine.state_machine_arn
        )
  ```

  </TabItem>
</Tabs>

### Define an EMR on EKS Spark Job

The stack defined below shows a usage example of the `EmrOnEksSparkJob` construct.

<Tabs>
  <TabItem value="typescript" label="TypeScript" default>

  ```typescript
  class ExampleSparkJobEmrEksStack extends cdk.Stack {
    constructor(scope: Construct, id: string) {
        super(scope, id);

        const dailyJob = new dsf.processing.SparkEmrEksJob(this, 'SparkNightlyJob', {
            name: 'daily_job',
            virtualClusterId: 'exampleId123',
            executionRoleArn: 'arn:aws:iam::123456789012:role/role',
            executionTimeoutMinutes: 30,
            s3LogUri: 's3://emr-job-logs-EXAMPLE/logs',
            sparkSubmitEntryPoint: 'local:///usr/lib/spark/examples/src/main/python/pi.py',
            sparkSubmitParameters: '--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4',
        });

        new CfnOutput(this, 'daily-job-state-machine', {
            value: dailyJob.stateMachine!.stateMachineArn,
        });
    }
}
  ```
  
  ```mdx-code-block
  
  </TabItem>
  <TabItem value="python" label="Python">

  ```python
  class ExampleSparkJobEmrEksStack(cdk.Stack):
    def __init__(self, scope, id):
        super().__init__(scope, id)

        daily_job = dsf.processing.SparkEmrEksJob(self, "SparkNightlyJob",
            name="daily_job",
            virtual_cluster_id="exampleId123",
            execution_role_arn="arn:aws:iam::123456789012:role/role",
            execution_timeout_minutes=30,
            s3_log_uri="s3://emr-job-logs-EXAMPLE/logs",
            spark_submit_entry_point="local:///usr/lib/spark/examples/src/main/python/pi.py",
            spark_submit_parameters="--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4"
        )

        CfnOutput(self, "daily-job-state-machine",
            value=daily_job.state_machine.state_machine_arn
        )
  ```

  </TabItem>
</Tabs>


