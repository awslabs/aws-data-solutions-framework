{"version":"2","toolVersion":"5.2.0","snippets":{"c8995391be7a07972184346db57b8969b3cc995d7a3a606111b4f8f011a40fc3":{"translations":{"python":{"source":"bucket = dsf.AccessLogsBucket(self, \"AccessLogsBucket\",\n    removal_policy=cdk.RemovalPolicy.DESTROY\n)","version":"2"},"csharp":{"source":"var bucket = new AccessLogsBucket(this, \"AccessLogsBucket\", new BucketProps {\n    RemovalPolicy = RemovalPolicy.DESTROY\n});","version":"1"},"java":{"source":"AccessLogsBucket bucket = AccessLogsBucket.Builder.create(this, \"AccessLogsBucket\")\n        .removalPolicy(RemovalPolicy.DESTROY)\n        .build();","version":"1"},"go":{"source":"bucket := dsf.NewAccessLogsBucket(this, jsii.String(\"AccessLogsBucket\"), &BucketProps{\n\tRemovalPolicy: cdk.RemovalPolicy_DESTROY,\n})","version":"1"},"$":{"source":"const bucket = new dsf.AccessLogsBucket(this, 'AccessLogsBucket', {\n removalPolicy: cdk.RemovalPolicy.DESTROY,\n})","version":"0"}},"location":{"api":{"api":"type","fqn":"aws-dsf.AccessLogsBucket"},"field":{"field":"example"}},"didCompile":true,"fqnsReferenced":["aws-cdk-lib.RemovalPolicy","aws-cdk-lib.RemovalPolicy#DESTROY","aws-cdk-lib.aws_s3.BucketProps","aws-dsf.AccessLogsBucket","constructs.Construct"],"fullSource":"import { Construct } from 'constructs';\nimport * as cdk from 'aws-cdk-lib';\nimport * as dsf from 'aws-dsf';\n\nclass MyStack extends cdk.Stack {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n\n    // Code snippet begins after !show marker below\n/// !show\nconst bucket = new dsf.AccessLogsBucket(this, 'AccessLogsBucket', {\n removalPolicy: cdk.RemovalPolicy.DESTROY,\n})\n/// !hide\n// Code snippet ended before !hide marker above\n  }\n}","syntaxKindCounter":{"11":1,"80":7,"110":1,"210":1,"211":3,"214":1,"243":1,"260":1,"261":1,"303":1},"fqnsFingerprint":"fabc0efd452169af338ebf46a8b8ea57a7accee2d7111c130bf958352a35c32a"},"254f12da6e0f7b7561ffc78cd974741ac46c7df7cd9db02f661d43edcd823864":{"translations":{"python":{"source":"from aws_cdk.aws_kms import Key\n\n\n# Set context value for global data removal policy (or set in cdk.json).\nself.node.set_context(\"@aws-data-solutions-framework/removeDataOnDestroy\", True)\n\nencryption_key = Key(self, \"DataKey\",\n    removal_policy=cdk.RemovalPolicy.DESTROY,\n    enable_key_rotation=True\n)\n\ndsf.AnalyticsBucket(self, \"MyAnalyticsBucket\",\n    encryption_key=encryption_key,\n    removal_policy=cdk.RemovalPolicy.DESTROY\n)","version":"2"},"csharp":{"source":"using Amazon.CDK.AWS.KMS;\n\n\n// Set context value for global data removal policy (or set in cdk.json).\nNode.SetContext(\"@aws-data-solutions-framework/removeDataOnDestroy\", true);\n\nvar encryptionKey = new Key(this, \"DataKey\", new KeyProps {\n    RemovalPolicy = RemovalPolicy.DESTROY,\n    EnableKeyRotation = true\n});\n\nnew AnalyticsBucket(this, \"MyAnalyticsBucket\", new AnalyticsBucketProps {\n    EncryptionKey = encryptionKey,\n    RemovalPolicy = RemovalPolicy.DESTROY\n});","version":"1"},"java":{"source":"import software.amazon.awscdk.services.kms.Key;\n\n\n// Set context value for global data removal policy (or set in cdk.json).\nthis.node.setContext(\"@aws-data-solutions-framework/removeDataOnDestroy\", true);\n\nKey encryptionKey = Key.Builder.create(this, \"DataKey\")\n        .removalPolicy(RemovalPolicy.DESTROY)\n        .enableKeyRotation(true)\n        .build();\n\nAnalyticsBucket.Builder.create(this, \"MyAnalyticsBucket\")\n        .encryptionKey(encryptionKey)\n        .removalPolicy(RemovalPolicy.DESTROY)\n        .build();","version":"1"},"go":{"source":"import \"github.com/aws/aws-cdk-go/awscdk\"\n\n\n// Set context value for global data removal policy (or set in cdk.json).\nthis.Node.SetContext(jsii.String(\"@aws-data-solutions-framework/removeDataOnDestroy\"), jsii.Boolean(true))\n\nencryptionKey := awscdk.NewKey(this, jsii.String(\"DataKey\"), &KeyProps{\n\tRemovalPolicy: cdk.RemovalPolicy_DESTROY,\n\tEnableKeyRotation: jsii.Boolean(true),\n})\n\ndsf.NewAnalyticsBucket(this, jsii.String(\"MyAnalyticsBucket\"), &AnalyticsBucketProps{\n\tEncryptionKey: EncryptionKey,\n\tRemovalPolicy: cdk.RemovalPolicy_DESTROY,\n})","version":"1"},"$":{"source":"import { Key } from 'aws-cdk-lib/aws-kms';\n\n// Set context value for global data removal policy (or set in cdk.json).\nthis.node.setContext('@aws-data-solutions-framework/removeDataOnDestroy', true);\n\nconst encryptionKey = new Key(this, 'DataKey', {\n removalPolicy: cdk.RemovalPolicy.DESTROY,\n enableKeyRotation: true,\n});\n\nnew dsf.AnalyticsBucket(this, 'MyAnalyticsBucket', {\n encryptionKey,\n removalPolicy: cdk.RemovalPolicy.DESTROY,\n});","version":"0"}},"location":{"api":{"api":"type","fqn":"aws-dsf.AnalyticsBucket"},"field":{"field":"example"}},"didCompile":true,"fqnsReferenced":["aws-cdk-lib.RemovalPolicy","aws-cdk-lib.RemovalPolicy#DESTROY","aws-cdk-lib.aws_kms.IKey","aws-cdk-lib.aws_kms.Key","aws-cdk-lib.aws_kms.KeyProps","aws-dsf.AnalyticsBucket","aws-dsf.AnalyticsBucketProps","constructs.Construct","constructs.Construct#node","constructs.Node#setContext"],"fullSource":"// Hoisted imports begin after !show marker below\n/// !show\nimport { Key } from 'aws-cdk-lib/aws-kms';\n/// !hide\n// Hoisted imports ended before !hide marker above\nimport { Construct } from 'constructs';\nimport * as cdk from 'aws-cdk-lib';\nimport * as dsf from 'aws-dsf';\n\nclass MyStack extends cdk.Stack {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n\n    // Code snippet begins after !show marker below\n/// !show\n\n\n// Set context value for global data removal policy (or set in cdk.json).\nthis.node.setContext('@aws-data-solutions-framework/removeDataOnDestroy', true);\n\nconst encryptionKey = new Key(this, 'DataKey', {\n removalPolicy: cdk.RemovalPolicy.DESTROY,\n enableKeyRotation: true,\n});\n\nnew dsf.AnalyticsBucket(this, 'MyAnalyticsBucket', {\n encryptionKey,\n removalPolicy: cdk.RemovalPolicy.DESTROY,\n});\n/// !hide\n// Code snippet ended before !hide marker above\n  }\n}","syntaxKindCounter":{"11":4,"80":17,"110":3,"112":2,"210":2,"211":7,"213":1,"214":2,"243":1,"244":2,"260":1,"261":1,"272":1,"273":1,"275":1,"276":1,"303":3,"304":1,"312":1},"fqnsFingerprint":"ad8ee6257bcb19c1ba3d3128e47967b906d80e22bfb2bd970117c1703168fe41"},"8f81e36acb8dea6012b402b169dcb8793a74e9f38ff809fbfea76066392d1e60":{"translations":{"python":{"source":"class MyApplicationStack(cdk.Stack):\n    def __init__(self, scope, id, *, stage, description=None, env=None, stackName=None, tags=None, synthesizer=None, terminationProtection=None, analyticsReporting=None, crossRegionReferences=None, permissionsBoundary=None, suppressTemplateIndentation=None):\n        super().__init__(scope, id, stage=stage, description=description, env=env, stackName=stackName, tags=tags, synthesizer=synthesizer, terminationProtection=terminationProtection, analyticsReporting=analyticsReporting, crossRegionReferences=crossRegionReferences, permissionsBoundary=permissionsBoundary, suppressTemplateIndentation=suppressTemplateIndentation)\n\nclass MyApplicationStackFactory(dsf.ApplicationStackFactory):\n    def create_stack(self, scope, stage):\n        return MyApplicationStack(scope, \"MyApplication\",\n            stage=stage\n        )","version":"2"},"csharp":{"source":"class MyApplicationStackProps : StackProps\n{\n    public CICDStage Stage { get; set; }\n}\n\nclass MyApplicationStack : Stack\n{\n    public MyApplicationStack(Construct scope, string id, MyApplicationStackProps? props=null) : base(scope, id, props)\n    {\n    }\n}\n\nclass MyApplicationStackFactory : ApplicationStackFactory\n{\n    public Stack CreateStack(Construct scope, CICDStage stage)\n    {\n        return new MyApplicationStack(scope, \"MyApplication\", (MyApplicationStackProps)new MyApplicationStackProps {\n            Stage = stage\n        });\n    }\n}","version":"1"},"java":{"source":"public class MyApplicationStackProps extends StackProps {\n    private CICDStage stage;\n    public CICDStage getStage() {\n        return this.stage;\n    }\n    public MyApplicationStackProps stage(CICDStage stage) {\n        this.stage = stage;\n        return this;\n    }\n}\n\npublic class MyApplicationStack extends Stack {\n    public MyApplicationStack(Construct scope, String id) {\n        this(scope, id, null);\n    }\n\n    public MyApplicationStack(Construct scope, String id, MyApplicationStackProps props) {\n        super(scope, id, props);\n    }\n}\n\npublic class MyApplicationStackFactory extends ApplicationStackFactory {\n    public Stack createStack(Construct scope, CICDStage stage) {\n        return new MyApplicationStack(scope, \"MyApplication\", (MyApplicationStackProps)new MyApplicationStackProps()\n                .stage(stage)\n                );\n    }\n}","version":"1"},"go":{"source":"type myApplicationStackProps struct {\n\tstackProps\n\tstage cICDStage\n}\n\ntype myApplicationStack struct {\n\tstack\n}\n\nfunc newMyApplicationStack(scope construct, id *string, props myApplicationStackProps) *myApplicationStack {\n\tthis := &myApplicationStack{}\n\tcdk.NewStack_Override(this, scope, id, props)\n\treturn this\n}\n\ntype myApplicationStackFactory struct {\n\tapplicationStackFactory\n}\n\nfunc (this *myApplicationStackFactory) createStack(scope construct, stage cICDStage) stack {\n\treturn NewMyApplicationStack(*scope, jsii.String(\"MyApplication\"), &myApplicationStackProps{\n\t\tstage: stage,\n\t}.(myApplicationStackProps))\n}","version":"1"},"$":{"source":"interface MyApplicationStackProps extends cdk.StackProps {\n  readonly stage: dsf.CICDStage;\n}\n\nclass MyApplicationStack extends cdk.Stack {\n  constructor(scope: Construct, id: string, props?: MyApplicationStackProps) {\n    super(scope, id, props);\n    // stack logic goes here... and can be customized using props.stage\n  }\n}\n\nclass MyApplicationStackFactory extends dsf.ApplicationStackFactory {\n  createStack(scope: Construct, stage: dsf.CICDStage): cdk.Stack {\n    return new MyApplicationStack(scope, 'MyApplication', {\n      stage: stage\n    } as MyApplicationStackProps);\n  }\n}","version":"0"}},"location":{"api":{"api":"type","fqn":"aws-dsf.ApplicationStackFactory"},"field":{"field":"example"}},"didCompile":true,"fqnsReferenced":["aws-cdk-lib.Stack","aws-cdk-lib.StackProps","aws-dsf.CICDStage","constructs.Construct"],"fullSource":"import { Construct } from 'constructs';\nimport * as cdk from 'aws-cdk-lib';\nimport * as dsf from 'aws-dsf';\n\n// Code snippet begins after !show marker below\n/// !show\ninterface MyApplicationStackProps extends cdk.StackProps {\n  readonly stage: dsf.CICDStage;\n}\n\nclass MyApplicationStack extends cdk.Stack {\n  constructor(scope: Construct, id: string, props?: MyApplicationStackProps) {\n    super(scope, id, props);\n    // stack logic goes here... and can be customized using props.stage\n  }\n}\n\nclass MyApplicationStackFactory extends dsf.ApplicationStackFactory {\n  createStack(scope: Construct, stage: dsf.CICDStage): cdk.Stack {\n    return new MyApplicationStack(scope, 'MyApplication', {\n      stage: stage\n    } as MyApplicationStackProps);\n  }\n}\n/// !hide\n// Code snippet ended before !hide marker above","syntaxKindCounter":{"11":1,"58":1,"80":33,"108":1,"148":1,"154":1,"166":3,"169":5,"171":1,"174":1,"176":1,"183":7,"210":1,"211":3,"213":1,"214":1,"233":3,"234":1,"241":2,"244":1,"253":1,"263":2,"264":1,"298":3,"303":1},"fqnsFingerprint":"bb39a26b572f30ea0df3a00c05edc55daa4b190f61a9f4ce6be5df79f2891097"},"e6d5a46e2a512a800f7076c7daac4906104f722fa88a7294e53e3ddfd8c7ba54":{"translations":{"python":{"source":"from aws_cdk.aws_s3 import Bucket\n\n\ndsf.DataCatalogDatabase(self, \"ExampleDatabase\",\n    location_bucket=Bucket(scope, \"LocationBucket\"),\n    location_prefix=\"/databasePath\",\n    name=\"example-db\"\n)","version":"2"},"csharp":{"source":"using Amazon.CDK.AWS.S3;\n\n\nnew DataCatalogDatabase(this, \"ExampleDatabase\", new DataCatalogDatabaseProps {\n    LocationBucket = new Bucket(scope, \"LocationBucket\"),\n    LocationPrefix = \"/databasePath\",\n    Name = \"example-db\"\n});","version":"1"},"java":{"source":"import software.amazon.awscdk.services.s3.Bucket;\n\n\nDataCatalogDatabase.Builder.create(this, \"ExampleDatabase\")\n        .locationBucket(new Bucket(scope, \"LocationBucket\"))\n        .locationPrefix(\"/databasePath\")\n        .name(\"example-db\")\n        .build();","version":"1"},"go":{"source":"import \"github.com/aws/aws-cdk-go/awscdk\"\n\n\ndsf.NewDataCatalogDatabase(this, jsii.String(\"ExampleDatabase\"), &DataCatalogDatabaseProps{\n\tLocationBucket: awscdk.NewBucket(scope, jsii.String(\"LocationBucket\")),\n\tLocationPrefix: jsii.String(\"/databasePath\"),\n\tName: jsii.String(\"example-db\"),\n})","version":"1"},"$":{"source":"import { Bucket } from 'aws-cdk-lib/aws-s3';\n\nnew dsf.DataCatalogDatabase(this, 'ExampleDatabase', {\n   locationBucket: new Bucket(scope, 'LocationBucket'),\n   locationPrefix: '/databasePath',\n   name: 'example-db'\n});","version":"0"}},"location":{"api":{"api":"type","fqn":"aws-dsf.DataCatalogDatabase"},"field":{"field":"example"}},"didCompile":true,"fqnsReferenced":["aws-cdk-lib.aws_s3.Bucket","aws-cdk-lib.aws_s3.IBucket","aws-dsf.DataCatalogDatabase","aws-dsf.DataCatalogDatabaseProps","constructs.Construct"],"fullSource":"// Hoisted imports begin after !show marker below\n/// !show\nimport { Bucket } from 'aws-cdk-lib/aws-s3';\n/// !hide\n// Hoisted imports ended before !hide marker above\nimport { Construct } from 'constructs';\nimport * as cdk from 'aws-cdk-lib';\nimport * as dsf from 'aws-dsf';\n\nclass MyStack extends cdk.Stack {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n\n    // Code snippet begins after !show marker below\n/// !show\n\n\nnew dsf.DataCatalogDatabase(this, 'ExampleDatabase', {\n   locationBucket: new Bucket(scope, 'LocationBucket'),\n   locationPrefix: '/databasePath',\n   name: 'example-db'\n});\n/// !hide\n// Code snippet ended before !hide marker above\n  }\n}","syntaxKindCounter":{"11":5,"80":8,"110":1,"210":1,"211":1,"214":2,"244":1,"272":1,"273":1,"275":1,"276":1,"303":3,"312":1},"fqnsFingerprint":"a5fef9f9568e341c6dd15db8cabf9ab7fd83e7143ba9b1d48aa796a83e67c829"},"c93e30e214a419bd429ea5a1c2b4cb9ba256e8f7bdaf77768d7f967ea88e7e82":{"translations":{"python":{"source":"from aws_cdk.aws_kms import Key\n\n\nlog_encryption_key = Key(self, \"LogEncryptionKey\")\nstorage = dsf.DataLakeStorage(self, \"ExampleStorage\")\ndata_lake_catalog = dsf.DataLakeCatalog(self, \"ExampleDataLakeCatalog\",\n    data_lake_storage=storage,\n    database_name=\"exampledb\",\n    crawler_log_encryption_key=log_encryption_key\n)","version":"2"},"csharp":{"source":"using Amazon.CDK.AWS.KMS;\n\n\nvar logEncryptionKey = new Key(this, \"LogEncryptionKey\");\nvar storage = new DataLakeStorage(this, \"ExampleStorage\");\nvar dataLakeCatalog = new DataLakeCatalog(this, \"ExampleDataLakeCatalog\", new DataLakeCatalogProps {\n    DataLakeStorage = storage,\n    DatabaseName = \"exampledb\",\n    CrawlerLogEncryptionKey = logEncryptionKey\n});","version":"1"},"java":{"source":"import software.amazon.awscdk.services.kms.Key;\n\n\nKey logEncryptionKey = new Key(this, \"LogEncryptionKey\");\nDataLakeStorage storage = new DataLakeStorage(this, \"ExampleStorage\");\nDataLakeCatalog dataLakeCatalog = DataLakeCatalog.Builder.create(this, \"ExampleDataLakeCatalog\")\n        .dataLakeStorage(storage)\n        .databaseName(\"exampledb\")\n        .crawlerLogEncryptionKey(logEncryptionKey)\n        .build();","version":"1"},"go":{"source":"import \"github.com/aws/aws-cdk-go/awscdk\"\n\n\nlogEncryptionKey := awscdk.NewKey(this, jsii.String(\"LogEncryptionKey\"))\nstorage := dsf.NewDataLakeStorage(this, jsii.String(\"ExampleStorage\"))\ndataLakeCatalog := dsf.NewDataLakeCatalog(this, jsii.String(\"ExampleDataLakeCatalog\"), &DataLakeCatalogProps{\n\tDataLakeStorage: storage,\n\tDatabaseName: jsii.String(\"exampledb\"),\n\tCrawlerLogEncryptionKey: logEncryptionKey,\n})","version":"1"},"$":{"source":"import { Key } from 'aws-cdk-lib/aws-kms';\n\nconst logEncryptionKey = new Key(this, 'LogEncryptionKey');\nconst storage = new dsf.DataLakeStorage(this, \"ExampleStorage\");\nconst dataLakeCatalog = new dsf.DataLakeCatalog(this, \"ExampleDataLakeCatalog\", {\n  dataLakeStorage: storage,\n  databaseName: \"exampledb\",\n  crawlerLogEncryptionKey: logEncryptionKey\n})","version":"0"}},"location":{"api":{"api":"type","fqn":"aws-dsf.DataLakeCatalog"},"field":{"field":"example"}},"didCompile":true,"fqnsReferenced":["aws-cdk-lib.aws_kms.Key","aws-dsf.DataLakeCatalog","aws-dsf.DataLakeCatalogProps","aws-dsf.DataLakeStorage","constructs.Construct"],"fullSource":"// Hoisted imports begin after !show marker below\n/// !show\nimport { Key } from 'aws-cdk-lib/aws-kms';\n/// !hide\n// Hoisted imports ended before !hide marker above\nimport { Construct } from 'constructs';\nimport * as cdk from 'aws-cdk-lib';\nimport * as dsf from 'aws-dsf';\n\nclass MyStack extends cdk.Stack {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n\n    // Code snippet begins after !show marker below\n/// !show\n\n\nconst logEncryptionKey = new Key(this, 'LogEncryptionKey');\nconst storage = new dsf.DataLakeStorage(this, \"ExampleStorage\");\nconst dataLakeCatalog = new dsf.DataLakeCatalog(this, \"ExampleDataLakeCatalog\", {\n  dataLakeStorage: storage,\n  databaseName: \"exampledb\",\n  crawlerLogEncryptionKey: logEncryptionKey\n})\n/// !hide\n// Code snippet ended before !hide marker above\n  }\n}","syntaxKindCounter":{"11":5,"80":14,"110":3,"210":1,"211":2,"214":3,"243":3,"260":3,"261":3,"272":1,"273":1,"275":1,"276":1,"303":3,"312":1},"fqnsFingerprint":"2098e2da6cc71e16a567f2ee131855917277518f256cec446acf6f91c5383235"},"3ba9a60ffc16bfa22a0d4b25193baead566061719399d75c20cdb3b940b2f040":{"translations":{"python":{"source":"# Set the context value for global data removal policy\nself.node.set_context(\"@aws-data-solutions-framework/removeDataOnDestroy\", True)\n\ndsf.DataLakeStorage(self, \"MyDataLakeStorage\",\n    bronze_bucket_name=\"my-bronze\",\n    bronze_bucket_infrequent_access_delay=90,\n    bronze_bucket_archive_delay=180,\n    silver_bucket_name=\"my-silver\",\n    silver_bucket_infrequent_access_delay=180,\n    silver_bucket_archive_delay=360,\n    gold_bucket_name=\"my-gold\",\n    gold_bucket_infrequent_access_delay=180,\n    gold_bucket_archive_delay=360,\n    removal_policy=cdk.RemovalPolicy.DESTROY\n)","version":"2"},"csharp":{"source":"// Set the context value for global data removal policy\nNode.SetContext(\"@aws-data-solutions-framework/removeDataOnDestroy\", true);\n\nnew DataLakeStorage(this, \"MyDataLakeStorage\", new DataLakeStorageProps {\n    BronzeBucketName = \"my-bronze\",\n    BronzeBucketInfrequentAccessDelay = 90,\n    BronzeBucketArchiveDelay = 180,\n    SilverBucketName = \"my-silver\",\n    SilverBucketInfrequentAccessDelay = 180,\n    SilverBucketArchiveDelay = 360,\n    GoldBucketName = \"my-gold\",\n    GoldBucketInfrequentAccessDelay = 180,\n    GoldBucketArchiveDelay = 360,\n    RemovalPolicy = RemovalPolicy.DESTROY\n});","version":"1"},"java":{"source":"// Set the context value for global data removal policy\nthis.node.setContext(\"@aws-data-solutions-framework/removeDataOnDestroy\", true);\n\nDataLakeStorage.Builder.create(this, \"MyDataLakeStorage\")\n        .bronzeBucketName(\"my-bronze\")\n        .bronzeBucketInfrequentAccessDelay(90)\n        .bronzeBucketArchiveDelay(180)\n        .silverBucketName(\"my-silver\")\n        .silverBucketInfrequentAccessDelay(180)\n        .silverBucketArchiveDelay(360)\n        .goldBucketName(\"my-gold\")\n        .goldBucketInfrequentAccessDelay(180)\n        .goldBucketArchiveDelay(360)\n        .removalPolicy(RemovalPolicy.DESTROY)\n        .build();","version":"1"},"go":{"source":"// Set the context value for global data removal policy\nthis.Node.SetContext(jsii.String(\"@aws-data-solutions-framework/removeDataOnDestroy\"), jsii.Boolean(true))\n\ndsf.NewDataLakeStorage(this, jsii.String(\"MyDataLakeStorage\"), &DataLakeStorageProps{\n\tBronzeBucketName: jsii.String(\"my-bronze\"),\n\tBronzeBucketInfrequentAccessDelay: jsii.Number(90),\n\tBronzeBucketArchiveDelay: jsii.Number(180),\n\tSilverBucketName: jsii.String(\"my-silver\"),\n\tSilverBucketInfrequentAccessDelay: jsii.Number(180),\n\tSilverBucketArchiveDelay: jsii.Number(360),\n\tGoldBucketName: jsii.String(\"my-gold\"),\n\tGoldBucketInfrequentAccessDelay: jsii.Number(180),\n\tGoldBucketArchiveDelay: jsii.Number(360),\n\tRemovalPolicy: cdk.RemovalPolicy_DESTROY,\n})","version":"1"},"$":{"source":"// Set the context value for global data removal policy\nthis.node.setContext('@aws-data-solutions-framework/removeDataOnDestroy', true);\n\nnew dsf.DataLakeStorage(this, 'MyDataLakeStorage', {\n bronzeBucketName: 'my-bronze',\n bronzeBucketInfrequentAccessDelay: 90,\n bronzeBucketArchiveDelay: 180,\n silverBucketName: 'my-silver',\n silverBucketInfrequentAccessDelay: 180,\n silverBucketArchiveDelay: 360,\n goldBucketName: 'my-gold',\n goldBucketInfrequentAccessDelay: 180,\n goldBucketArchiveDelay: 360,\n removalPolicy: cdk.RemovalPolicy.DESTROY,\n});","version":"0"}},"location":{"api":{"api":"type","fqn":"aws-dsf.DataLakeStorage"},"field":{"field":"example"}},"didCompile":true,"fqnsReferenced":["aws-cdk-lib.RemovalPolicy","aws-cdk-lib.RemovalPolicy#DESTROY","aws-dsf.DataLakeStorage","aws-dsf.DataLakeStorageProps","constructs.Construct","constructs.Construct#node","constructs.Node#setContext"],"fullSource":"import { Construct } from 'constructs';\nimport * as cdk from 'aws-cdk-lib';\nimport * as dsf from 'aws-dsf';\n\nclass MyStack extends cdk.Stack {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n\n    // Code snippet begins after !show marker below\n/// !show\n// Set the context value for global data removal policy\nthis.node.setContext('@aws-data-solutions-framework/removeDataOnDestroy', true);\n\nnew dsf.DataLakeStorage(this, 'MyDataLakeStorage', {\n bronzeBucketName: 'my-bronze',\n bronzeBucketInfrequentAccessDelay: 90,\n bronzeBucketArchiveDelay: 180,\n silverBucketName: 'my-silver',\n silverBucketInfrequentAccessDelay: 180,\n silverBucketArchiveDelay: 360,\n goldBucketName: 'my-gold',\n goldBucketInfrequentAccessDelay: 180,\n goldBucketArchiveDelay: 360,\n removalPolicy: cdk.RemovalPolicy.DESTROY,\n});\n/// !hide\n// Code snippet ended before !hide marker above\n  }\n}","syntaxKindCounter":{"9":6,"11":5,"80":17,"110":2,"112":1,"210":1,"211":5,"213":1,"214":1,"244":2,"303":10},"fqnsFingerprint":"eb02b1c74a9896ca83a0b186754ecbff10528129dc27a0113b005147795e28a2"},"428c94494cad3f7dfd581109a6bec08928cd4fe550cd6c58e1d6d95d0a996e69":{"translations":{"python":{"source":"pyspark_packer = dsf.PySparkApplicationPackage(self, \"pysparkPacker\",\n    application_name=\"my-pyspark\",\n    entrypoint_path=\"/Users/my-user/my-spark-job/app/app-pyspark.py\",\n    dependencies_folder=\"/Users/my-user/my-spark-job/app\",\n    removal_policy=cdk.RemovalPolicy.DESTROY\n)\n\nspark_env_conf = f\"--conf spark.archives={pysparkPacker.venvArchiveS3Uri} --conf spark.emr-serverless.driverEnv.PYSPARK_DRIVER_PYTHON=./environment/bin/python --conf spark.emr-serverless.driverEnv.PYSPARK_PYTHON=./environment/bin/python --conf spark.emr-serverless.executorEnv.PYSPARK_PYTHON=./environment/bin/python\"\n\ndsf.SparkEmrServerlessJob(self, \"SparkJobServerless\",\n    name=\"MyPySpark\",\n    application_id=\"xxxxxxxxx\",\n    execution_role_arn=\"ROLE-ARN\",\n    execution_timeout_minutes=30,\n    s3_log_uri=\"s3://s3-bucket/monitoring-logs\",\n    cloud_watch_log_group_name=\"my-pyspark-serverless-log\",\n    spark_submit_entry_point=f\"{pysparkPacker.entrypointS3Uri}\",\n    spark_submit_parameters=f\"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4 {sparkEnvConf}\"\n)","version":"2"},"csharp":{"source":"var pysparkPacker = new PySparkApplicationPackage(this, \"pysparkPacker\", new PySparkApplicationPackageProps {\n    ApplicationName = \"my-pyspark\",\n    EntrypointPath = \"/Users/my-user/my-spark-job/app/app-pyspark.py\",\n    DependenciesFolder = \"/Users/my-user/my-spark-job/app\",\n    RemovalPolicy = RemovalPolicy.DESTROY\n});\n\nvar sparkEnvConf = $\"--conf spark.archives={pysparkPacker.venvArchiveS3Uri} --conf spark.emr-serverless.driverEnv.PYSPARK_DRIVER_PYTHON=./environment/bin/python --conf spark.emr-serverless.driverEnv.PYSPARK_PYTHON=./environment/bin/python --conf spark.emr-serverless.executorEnv.PYSPARK_PYTHON=./environment/bin/python\";\n\nnew SparkEmrServerlessJob(this, \"SparkJobServerless\", (SparkEmrServerlessJobProps)new SparkEmrServerlessJobProps {\n    Name = \"MyPySpark\",\n    ApplicationId = \"xxxxxxxxx\",\n    ExecutionRoleArn = \"ROLE-ARN\",\n    ExecutionTimeoutMinutes = 30,\n    S3LogUri = \"s3://s3-bucket/monitoring-logs\",\n    CloudWatchLogGroupName = \"my-pyspark-serverless-log\",\n    SparkSubmitEntryPoint = $\"{pysparkPacker.entrypointS3Uri}\",\n    SparkSubmitParameters = $\"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4 {sparkEnvConf}\"\n});","version":"1"},"java":{"source":"PySparkApplicationPackage pysparkPacker = PySparkApplicationPackage.Builder.create(this, \"pysparkPacker\")\n        .applicationName(\"my-pyspark\")\n        .entrypointPath(\"/Users/my-user/my-spark-job/app/app-pyspark.py\")\n        .dependenciesFolder(\"/Users/my-user/my-spark-job/app\")\n        .removalPolicy(RemovalPolicy.DESTROY)\n        .build();\n\nString sparkEnvConf = String.format(\"--conf spark.archives=%s --conf spark.emr-serverless.driverEnv.PYSPARK_DRIVER_PYTHON=./environment/bin/python --conf spark.emr-serverless.driverEnv.PYSPARK_PYTHON=./environment/bin/python --conf spark.emr-serverless.executorEnv.PYSPARK_PYTHON=./environment/bin/python\", pysparkPacker.getVenvArchiveS3Uri());\n\nnew SparkEmrServerlessJob(this, \"SparkJobServerless\", (SparkEmrServerlessJobProps)SparkEmrServerlessJobProps.builder()\n        .name(\"MyPySpark\")\n        .applicationId(\"xxxxxxxxx\")\n        .executionRoleArn(\"ROLE-ARN\")\n        .executionTimeoutMinutes(30)\n        .s3LogUri(\"s3://s3-bucket/monitoring-logs\")\n        .cloudWatchLogGroupName(\"my-pyspark-serverless-log\")\n        .sparkSubmitEntryPoint(String.format(\"%s\", pysparkPacker.getEntrypointS3Uri()))\n        .sparkSubmitParameters(String.format(\"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4 %s\", sparkEnvConf))\n        .build());","version":"1"},"go":{"source":"pysparkPacker := dsf.NewPySparkApplicationPackage(this, jsii.String(\"pysparkPacker\"), &PySparkApplicationPackageProps{\n\tApplicationName: jsii.String(\"my-pyspark\"),\n\tEntrypointPath: jsii.String(\"/Users/my-user/my-spark-job/app/app-pyspark.py\"),\n\tDependenciesFolder: jsii.String(\"/Users/my-user/my-spark-job/app\"),\n\tRemovalPolicy: cdk.RemovalPolicy_DESTROY,\n})\n\nsparkEnvConf := fmt.Sprintf(\"--conf spark.archives=%v --conf spark.emr-serverless.driverEnv.PYSPARK_DRIVER_PYTHON=./environment/bin/python --conf spark.emr-serverless.driverEnv.PYSPARK_PYTHON=./environment/bin/python --conf spark.emr-serverless.executorEnv.PYSPARK_PYTHON=./environment/bin/python\", pysparkPacker.VenvArchiveS3Uri)\n\ndsf.NewSparkEmrServerlessJob(this, jsii.String(\"SparkJobServerless\"), &SparkEmrServerlessJobProps{\n\tName: jsii.String(\"MyPySpark\"),\n\tApplicationId: jsii.String(\"xxxxxxxxx\"),\n\tExecutionRoleArn: jsii.String(\"ROLE-ARN\"),\n\tExecutionTimeoutMinutes: jsii.Number(30),\n\tS3LogUri: jsii.String(\"s3://s3-bucket/monitoring-logs\"),\n\tCloudWatchLogGroupName: jsii.String(\"my-pyspark-serverless-log\"),\n\tSparkSubmitEntryPoint: fmt.Sprintf(\"%v\", pysparkPacker.EntrypointS3Uri),\n\tSparkSubmitParameters: fmt.Sprintf(\"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4 %v\", sparkEnvConf),\n}.(sparkEmrServerlessJobProps))","version":"1"},"$":{"source":"let pysparkPacker = new dsf.PySparkApplicationPackage (this, 'pysparkPacker', {\n  applicationName: 'my-pyspark',\n  entrypointPath: '/Users/my-user/my-spark-job/app/app-pyspark.py',\n  dependenciesFolder: '/Users/my-user/my-spark-job/app',\n  removalPolicy: cdk.RemovalPolicy.DESTROY,\n});\n\nlet sparkEnvConf: string = `--conf spark.archives=${pysparkPacker.venvArchiveS3Uri} --conf spark.emr-serverless.driverEnv.PYSPARK_DRIVER_PYTHON=./environment/bin/python --conf spark.emr-serverless.driverEnv.PYSPARK_PYTHON=./environment/bin/python --conf spark.emr-serverless.executorEnv.PYSPARK_PYTHON=./environment/bin/python`\n\nnew dsf.SparkEmrServerlessJob(this, 'SparkJobServerless', {\n  name: 'MyPySpark',\n  applicationId: 'xxxxxxxxx',\n  executionRoleArn: 'ROLE-ARN',\n  executionTimeoutMinutes: 30,\n  s3LogUri: 's3://s3-bucket/monitoring-logs',\n  cloudWatchLogGroupName: 'my-pyspark-serverless-log',\n  sparkSubmitEntryPoint: `${pysparkPacker.entrypointS3Uri}`,\n  sparkSubmitParameters: `--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4 ${sparkEnvConf}`,\n} as dsf.SparkEmrServerlessJobProps);","version":"0"}},"location":{"api":{"api":"type","fqn":"aws-dsf.PySparkApplicationPackage"},"field":{"field":"example"}},"didCompile":true,"fqnsReferenced":["aws-cdk-lib.RemovalPolicy","aws-cdk-lib.RemovalPolicy#DESTROY","aws-dsf.PySparkApplicationPackage","aws-dsf.PySparkApplicationPackageProps","aws-dsf.SparkEmrServerlessJob","constructs.Construct"],"fullSource":"import { Construct } from 'constructs';\nimport * as cdk from 'aws-cdk-lib';\nimport * as dsf from 'aws-dsf';\n\nclass MyStack extends cdk.Stack {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n\n    // Code snippet begins after !show marker below\n/// !show\nlet pysparkPacker = new dsf.PySparkApplicationPackage (this, 'pysparkPacker', {\n  applicationName: 'my-pyspark',\n  entrypointPath: '/Users/my-user/my-spark-job/app/app-pyspark.py',\n  dependenciesFolder: '/Users/my-user/my-spark-job/app',\n  removalPolicy: cdk.RemovalPolicy.DESTROY,\n});\n\nlet sparkEnvConf: string = `--conf spark.archives=${pysparkPacker.venvArchiveS3Uri} --conf spark.emr-serverless.driverEnv.PYSPARK_DRIVER_PYTHON=./environment/bin/python --conf spark.emr-serverless.driverEnv.PYSPARK_PYTHON=./environment/bin/python --conf spark.emr-serverless.executorEnv.PYSPARK_PYTHON=./environment/bin/python`\n\nnew dsf.SparkEmrServerlessJob(this, 'SparkJobServerless', {\n  name: 'MyPySpark',\n  applicationId: 'xxxxxxxxx',\n  executionRoleArn: 'ROLE-ARN',\n  executionTimeoutMinutes: 30,\n  s3LogUri: 's3://s3-bucket/monitoring-logs',\n  cloudWatchLogGroupName: 'my-pyspark-serverless-log',\n  sparkSubmitEntryPoint: `${pysparkPacker.entrypointS3Uri}`,\n  sparkSubmitParameters: `--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4 ${sparkEnvConf}`,\n} as dsf.SparkEmrServerlessJobProps);\n/// !hide\n// Code snippet ended before !hide marker above\n  }\n}","syntaxKindCounter":{"9":1,"11":10,"16":3,"18":3,"80":28,"110":2,"154":1,"166":1,"183":1,"210":2,"211":6,"214":2,"228":3,"234":1,"239":3,"243":2,"244":1,"260":2,"261":2,"303":12},"fqnsFingerprint":"72b61f02c5e909b65f1d239f69ed4486421f842a460fc400a0274b29312cf04f"},"6c9897309461e55835f0f46b845ed299db70efded198e8ea8c6db82103ff5d18":{"translations":{"python":{"source":"from aws_cdk.aws_s3 import Bucket\n\n\nclass MyApplicationStack(cdk.Stack):\n    def __init__(self, scope, *, stage, description=None, env=None, stackName=None, tags=None, synthesizer=None, terminationProtection=None, analyticsReporting=None, crossRegionReferences=None, permissionsBoundary=None, suppressTemplateIndentation=None):\n        super().__init__(scope, \"MyApplicationStack\")\n        bucket = Bucket(self, \"TestBucket\",\n            auto_delete_objects=True,\n            removal_policy=cdk.RemovalPolicy.DESTROY\n        )\n        cdk.CfnOutput(self, \"BucketName\", value=bucket.bucket_name)\n\nclass MyStackFactory(dsf.ApplicationStackFactory):\n    def create_stack(self, scope, stage):\n        return MyApplicationStack(scope, stage=stage)\n\nclass MyCICDStack(cdk.Stack):\n    def __init__(self, scope, id):\n        super().__init__(scope, id)\n        dsf.SparkEmrCICDPipeline(self, \"TestConstruct\",\n            spark_application_name=\"test\",\n            application_stack_factory=MyStackFactory(),\n            cdk_application_path=\"cdk/\",\n            spark_application_path=\"spark/\",\n            spark_image=dsf.SparkImage.EMR_6_12,\n            integ_test_script=\"cdk/integ-test.sh\",\n            integ_test_env={\n                \"TEST_BUCKET\": \"BucketName\"\n            }\n        )","version":"2"},"csharp":{"source":"using Amazon.CDK.AWS.S3;\n\n\nclass MyApplicationStackProps : StackProps\n{\n    public CICDStage Stage { get; set; }\n}\n\nclass MyApplicationStack : Stack\n{\n    public MyApplicationStack(Stack scope, MyApplicationStackProps? props=null) : base(scope, \"MyApplicationStack\")\n    {\n        var bucket = new Bucket(this, \"TestBucket\", new BucketProps {\n            AutoDeleteObjects = true,\n            RemovalPolicy = RemovalPolicy.DESTROY\n        });\n        new CfnOutput(this, \"BucketName\", new CfnOutputProps { Value = bucket.BucketName });\n    }\n}\n\nclass MyStackFactory : ApplicationStackFactory\n{\n    public Stack CreateStack(Stack scope, CICDStage stage)\n    {\n        return new MyApplicationStack(scope, new MyApplicationStackProps { Stage = stage });\n    }\n}\n\nclass MyCICDStack : Stack\n{\n    public MyCICDStack(Construct scope, string id) : base(scope, id)\n    {\n        new SparkEmrCICDPipeline(this, \"TestConstruct\", new SparkEmrCICDPipelineProps {\n            SparkApplicationName = \"test\",\n            ApplicationStackFactory = new MyStackFactory(),\n            CdkApplicationPath = \"cdk/\",\n            SparkApplicationPath = \"spark/\",\n            SparkImage = SparkImage.EMR_6_12,\n            IntegTestScript = \"cdk/integ-test.sh\",\n            IntegTestEnv = new Dictionary<string, string> {\n                { \"TEST_BUCKET\", \"BucketName\" }\n            }\n        });\n    }\n}","version":"1"},"java":{"source":"import software.amazon.awscdk.services.s3.Bucket;\n\n\npublic class MyApplicationStackProps extends StackProps {\n    private CICDStage stage;\n    public CICDStage getStage() {\n        return this.stage;\n    }\n    public MyApplicationStackProps stage(CICDStage stage) {\n        this.stage = stage;\n        return this;\n    }\n}\n\npublic class MyApplicationStack extends Stack {\n    public MyApplicationStack(Stack scope) {\n        this(scope, null);\n    }\n\n    public MyApplicationStack(Stack scope, MyApplicationStackProps props) {\n        super(scope, \"MyApplicationStack\");\n        Bucket bucket = Bucket.Builder.create(this, \"TestBucket\")\n                .autoDeleteObjects(true)\n                .removalPolicy(RemovalPolicy.DESTROY)\n                .build();\n        CfnOutput.Builder.create(this, \"BucketName\").value(bucket.getBucketName()).build();\n    }\n}\n\npublic class MyStackFactory implements ApplicationStackFactory {\n    public Stack createStack(Stack scope, CICDStage stage) {\n        return new MyApplicationStack(scope, new MyApplicationStackProps().stage(stage));\n    }\n}\n\npublic class MyCICDStack extends Stack {\n    public MyCICDStack(Construct scope, String id) {\n        super(scope, id);\n        SparkEmrCICDPipeline.Builder.create(this, \"TestConstruct\")\n                .sparkApplicationName(\"test\")\n                .applicationStackFactory(new MyStackFactory())\n                .cdkApplicationPath(\"cdk/\")\n                .sparkApplicationPath(\"spark/\")\n                .sparkImage(SparkImage.EMR_6_12)\n                .integTestScript(\"cdk/integ-test.sh\")\n                .integTestEnv(Map.of(\n                        \"TEST_BUCKET\", \"BucketName\"))\n                .build();\n    }\n}","version":"1"},"go":{"source":"import \"github.com/aws/aws-cdk-go/awscdk\"\n\n\ntype myApplicationStackProps struct {\n\tstackProps\n\tstage cICDStage\n}\n\ntype myApplicationStack struct {\n\tstack\n}\n\nfunc newMyApplicationStack(scope stack, props myApplicationStackProps) *myApplicationStack {\n\tthis := &myApplicationStack{}\n\tcdk.NewStack_Override(this, scope, jsii.String(\"MyApplicationStack\"))\n\tbucket := awscdk.NewBucket(this, jsii.String(\"TestBucket\"), &BucketProps{\n\t\tAutoDeleteObjects: jsii.Boolean(true),\n\t\tRemovalPolicy: cdk.RemovalPolicy_DESTROY,\n\t})\n\tcdk.NewCfnOutput(this, jsii.String(\"BucketName\"), &CfnOutputProps{\n\t\tValue: bucket.BucketName,\n\t})\n\treturn this\n}\n\ntype myStackFactory struct {\n}\n\nfunc (this *myStackFactory) createStack(scope stack, stage cICDStage) stack {\n\treturn NewMyApplicationStack(*scope, &myApplicationStackProps{\n\t\tstage: stage,\n\t})\n}\n\ntype myCICDStack struct {\n\tstack\n}\n\nfunc newMyCICDStack(scope construct, id *string) *myCICDStack {\n\tthis := &myCICDStack{}\n\tcdk.NewStack_Override(this, scope, id)\n\tdsf.NewSparkEmrCICDPipeline(this, jsii.String(\"TestConstruct\"), &SparkEmrCICDPipelineProps{\n\t\tSparkApplicationName: jsii.String(\"test\"),\n\t\tApplicationStackFactory: NewMyStackFactory(),\n\t\tCdkApplicationPath: jsii.String(\"cdk/\"),\n\t\tSparkApplicationPath: jsii.String(\"spark/\"),\n\t\tSparkImage: dsf.SparkImage_EMR_6_12,\n\t\tIntegTestScript: jsii.String(\"cdk/integ-test.sh\"),\n\t\tIntegTestEnv: map[string]*string{\n\t\t\t\"TEST_BUCKET\": jsii.String(\"BucketName\"),\n\t\t},\n\t})\n\treturn this\n}","version":"1"},"$":{"source":"import { Bucket } from 'aws-cdk-lib/aws-s3';\n\ninterface MyApplicationStackProps extends cdk.StackProps {\n  readonly stage: dsf.CICDStage;\n}\n\nclass MyApplicationStack extends cdk.Stack {\n  constructor(scope: cdk.Stack, props?: MyApplicationStackProps) {\n    super(scope, 'MyApplicationStack');\n    const bucket = new Bucket(this, 'TestBucket', {\n      autoDeleteObjects: true,\n      removalPolicy: cdk.RemovalPolicy.DESTROY,\n    });\n    new cdk.CfnOutput(this, 'BucketName', { value: bucket.bucketName });\n  }\n}\n\nclass MyStackFactory implements dsf.ApplicationStackFactory {\n  createStack(scope: cdk.Stack, stage: dsf.CICDStage): cdk.Stack {\n    return new MyApplicationStack(scope, { stage });\n  }\n}\n\nclass MyCICDStack extends cdk.Stack {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n    new dsf.SparkEmrCICDPipeline(this, 'TestConstruct', {\n       sparkApplicationName: 'test',\n       applicationStackFactory: new MyStackFactory(),\n       cdkApplicationPath: 'cdk/',\n       sparkApplicationPath: 'spark/',\n       sparkImage: dsf.SparkImage.EMR_6_12,\n       integTestScript: 'cdk/integ-test.sh',\n       integTestEnv: {\n         TEST_BUCKET: 'BucketName',\n       },\n    });\n  }\n}","version":"0"}},"location":{"api":{"api":"type","fqn":"aws-dsf.SparkEmrCICDPipeline"},"field":{"field":"example"}},"didCompile":true,"fqnsReferenced":["aws-cdk-lib.CfnOutput","aws-cdk-lib.CfnOutputProps","aws-cdk-lib.RemovalPolicy","aws-cdk-lib.RemovalPolicy#DESTROY","aws-cdk-lib.Stack","aws-cdk-lib.aws_s3.Bucket","aws-cdk-lib.aws_s3.Bucket#bucketName","aws-cdk-lib.aws_s3.BucketProps","aws-dsf.ApplicationStackFactory","aws-dsf.CICDStage","aws-dsf.SparkEmrCICDPipeline","aws-dsf.SparkEmrCICDPipelineProps","aws-dsf.SparkImage","aws-dsf.SparkImage#EMR_6_12","constructs.Construct"],"fullSource":"// Hoisted imports begin after !show marker below\n/// !show\nimport { Bucket } from 'aws-cdk-lib/aws-s3';\n/// !hide\n// Hoisted imports ended before !hide marker above\nimport { Construct } from 'constructs';\nimport * as cdk from 'aws-cdk-lib';\nimport * as dsf from 'aws-dsf';\n\n// Code snippet begins after !show marker below\n/// !show\n\n\ninterface MyApplicationStackProps extends cdk.StackProps {\n  readonly stage: dsf.CICDStage;\n}\n\nclass MyApplicationStack extends cdk.Stack {\n  constructor(scope: cdk.Stack, props?: MyApplicationStackProps) {\n    super(scope, 'MyApplicationStack');\n    const bucket = new Bucket(this, 'TestBucket', {\n      autoDeleteObjects: true,\n      removalPolicy: cdk.RemovalPolicy.DESTROY,\n    });\n    new cdk.CfnOutput(this, 'BucketName', { value: bucket.bucketName });\n  }\n}\n\nclass MyStackFactory implements dsf.ApplicationStackFactory {\n  createStack(scope: cdk.Stack, stage: dsf.CICDStage): cdk.Stack {\n    return new MyApplicationStack(scope, { stage });\n  }\n}\n\nclass MyCICDStack extends cdk.Stack {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n    new dsf.SparkEmrCICDPipeline(this, 'TestConstruct', {\n       sparkApplicationName: 'test',\n       applicationStackFactory: new MyStackFactory(),\n       cdkApplicationPath: 'cdk/',\n       sparkApplicationPath: 'spark/',\n       sparkImage: dsf.SparkImage.EMR_6_12,\n       integTestScript: 'cdk/integ-test.sh',\n       integTestEnv: {\n         TEST_BUCKET: 'BucketName',\n       },\n    });\n  }\n}\n/// !hide\n// Code snippet ended before !hide marker above","syntaxKindCounter":{"11":10,"58":1,"80":65,"108":2,"110":3,"112":1,"148":1,"154":1,"166":5,"169":6,"171":1,"174":1,"176":2,"183":7,"210":5,"211":11,"213":2,"214":5,"233":4,"241":3,"243":1,"244":4,"253":1,"260":1,"261":1,"263":3,"264":1,"272":1,"273":1,"275":1,"276":1,"298":4,"303":11,"304":1,"312":1},"fqnsFingerprint":"61d0e7a6505c03830dd21b680ff386d5da35bacf63a688f809f14cd066b4b223"},"9c43df59215987416cc46c215538089fdfce506400d62a0537ad7239c49fcc42":{"translations":{"python":{"source":"from aws_cdk.aws_stepfunctions import JsonPath\n\n\njob = dsf.SparkEmrEksJob(self, \"SparkJob\",\n    job_config={\n        \"Name\": JsonPath.format(\"ge_profile-{}\", JsonPath.uuid()),\n        \"VirtualClusterId\": \"virtualClusterId\",\n        \"ExecutionRoleArn\": \"ROLE-ARN\",\n        \"JobDriver\": {\n            \"SparkSubmit\": {\n                \"EntryPoint\": \"s3://S3-BUCKET/pi.py\",\n                \"EntryPointArguments\": [],\n                \"SparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\"\n            }\n        }\n    }\n)\n\ncdk.CfnOutput(self, \"SparkJobStateMachine\",\n    value=job.state_machine.state_machine_arn\n)","version":"2"},"csharp":{"source":"using Amazon.CDK.AWS.StepFunctions;\n\n\nvar job = new SparkEmrEksJob(this, \"SparkJob\", (SparkEmrEksJobApiProps)new SparkEmrEksJobApiProps {\n    JobConfig = new Dictionary<string, object> {\n        { \"Name\", JsonPath.Format(\"ge_profile-{}\", JsonPath.Uuid()) },\n        { \"VirtualClusterId\", \"virtualClusterId\" },\n        { \"ExecutionRoleArn\", \"ROLE-ARN\" },\n        { \"JobDriver\", new Dictionary<string, IDictionary<string, object>> {\n            { \"SparkSubmit\", new Struct {\n                EntryPoint = \"s3://S3-BUCKET/pi.py\",\n                EntryPointArguments = new [] {  },\n                SparkSubmitParameters = \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\"\n            } }\n        } }\n    }\n});\n\nnew CfnOutput(this, \"SparkJobStateMachine\", new CfnOutputProps {\n    Value = job.StateMachine.StateMachineArn\n});","version":"1"},"java":{"source":"import software.amazon.awscdk.services.stepfunctions.JsonPath;\n\n\nSparkEmrEksJob job = new SparkEmrEksJob(this, \"SparkJob\", (SparkEmrEksJobApiProps)SparkEmrEksJobApiProps.builder()\n        .jobConfig(Map.of(\n                \"Name\", JsonPath.format(\"ge_profile-{}\", JsonPath.uuid()),\n                \"VirtualClusterId\", \"virtualClusterId\",\n                \"ExecutionRoleArn\", \"ROLE-ARN\",\n                \"JobDriver\", Map.of(\n                        \"SparkSubmit\", Map.of(\n                                \"EntryPoint\", \"s3://S3-BUCKET/pi.py\",\n                                \"EntryPointArguments\", List.of(),\n                                \"SparkSubmitParameters\", \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\"))))\n        .build());\n\nCfnOutput.Builder.create(this, \"SparkJobStateMachine\")\n        .value(job.getStateMachine().getStateMachineArn())\n        .build();","version":"1"},"go":{"source":"import \"github.com/aws/aws-cdk-go/awscdk\"\n\n\njob := dsf.NewSparkEmrEksJob(this, jsii.String(\"SparkJob\"), &SparkEmrEksJobApiProps{\n\tJobConfig: map[string]interface{}{\n\t\t\"Name\": awscdk.JsonPath_format(jsii.String(\"ge_profile-{}\"), awscdk.JsonPath_uuid()),\n\t\t\"VirtualClusterId\": jsii.String(\"virtualClusterId\"),\n\t\t\"ExecutionRoleArn\": jsii.String(\"ROLE-ARN\"),\n\t\t\"JobDriver\": map[string]map[string]interface{}{\n\t\t\t\"SparkSubmit\": map[string]interface{}{\n\t\t\t\t\"EntryPoint\": jsii.String(\"s3://S3-BUCKET/pi.py\"),\n\t\t\t\t\"EntryPointArguments\": []interface{}{\n\t\t\t\t},\n\t\t\t\t\"SparkSubmitParameters\": jsii.String(\"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\"),\n\t\t\t},\n\t\t},\n\t},\n}.(sparkEmrEksJobApiProps))\n\ncdk.NewCfnOutput(this, jsii.String(\"SparkJobStateMachine\"), &CfnOutputProps{\n\tValue: job.StateMachine.StateMachineArn,\n})","version":"1"},"$":{"source":"import { JsonPath } from 'aws-cdk-lib/aws-stepfunctions';\n\nconst job = new dsf.SparkEmrEksJob(this, 'SparkJob', {\n  jobConfig:{\n    \"Name\": JsonPath.format('ge_profile-{}', JsonPath.uuid()),\n    \"VirtualClusterId\": \"virtualClusterId\",\n    \"ExecutionRoleArn\": \"ROLE-ARN\",\n    \"JobDriver\": {\n      \"SparkSubmit\": {\n          \"EntryPoint\": \"s3://S3-BUCKET/pi.py\",\n          \"EntryPointArguments\": [],\n          \"SparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\"\n      },\n    }\n  }\n} as dsf.SparkEmrEksJobApiProps);\n\nnew cdk.CfnOutput(this, 'SparkJobStateMachine', {\n  value: job.stateMachine!.stateMachineArn,\n});","version":"0"}},"location":{"api":{"api":"type","fqn":"aws-dsf.SparkEmrEksJob"},"field":{"field":"example"}},"didCompile":true,"fqnsReferenced":["aws-cdk-lib.CfnOutput","aws-cdk-lib.CfnOutputProps","aws-cdk-lib.aws_stepfunctions.JsonPath#format","aws-cdk-lib.aws_stepfunctions.JsonPath#uuid","aws-cdk-lib.aws_stepfunctions.StateMachine#stateMachineArn","aws-dsf.SparkEmrEksJob","aws-dsf.SparkJob#stateMachine","constructs.Construct"],"fullSource":"// Hoisted imports begin after !show marker below\n/// !show\nimport { JsonPath } from 'aws-cdk-lib/aws-stepfunctions';\n/// !hide\n// Hoisted imports ended before !hide marker above\nimport { Construct } from 'constructs';\nimport * as cdk from 'aws-cdk-lib';\nimport * as dsf from 'aws-dsf';\n\nclass MyStack extends cdk.Stack {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n\n    // Code snippet begins after !show marker below\n/// !show\n\n\nconst job = new dsf.SparkEmrEksJob(this, 'SparkJob', {\n  jobConfig:{\n    \"Name\": JsonPath.format('ge_profile-{}', JsonPath.uuid()),\n    \"VirtualClusterId\": \"virtualClusterId\",\n    \"ExecutionRoleArn\": \"ROLE-ARN\",\n    \"JobDriver\": {\n      \"SparkSubmit\": {\n          \"EntryPoint\": \"s3://S3-BUCKET/pi.py\",\n          \"EntryPointArguments\": [],\n          \"SparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\"\n      },\n    }\n  }\n} as dsf.SparkEmrEksJobApiProps);\n\nnew cdk.CfnOutput(this, 'SparkJobStateMachine', {\n  value: job.stateMachine!.stateMachineArn,\n});\n/// !hide\n// Code snippet ended before !hide marker above\n  }\n}","syntaxKindCounter":{"11":16,"80":17,"110":2,"166":1,"183":1,"209":1,"210":5,"211":6,"213":2,"214":2,"234":1,"235":1,"243":1,"244":1,"260":1,"261":1,"272":1,"273":1,"275":1,"276":1,"303":10,"312":1},"fqnsFingerprint":"83d6bc430599ab5cf7ee300862687e8697af25d0629585ab0a5a102c9ef29e6b"},"6972e0d1b5cc2c2fd9de0bb9671779f0caff8dc8d3c773e4f208d71f44826aac":{"translations":{"python":{"source":"from aws_cdk.aws_iam import PolicyDocument, PolicyStatement\nfrom aws_cdk.aws_stepfunctions import JsonPath\n\n\nmy_file_system_policy = PolicyDocument(\n    statements=[PolicyStatement(\n        actions=[\"s3:GetObject\"\n        ],\n        resources=[\"*\"]\n    )]\n)\n\nmy_execution_role = dsf.SparkEmrServerlessRuntime.create_execution_role(self, \"execRole1\", my_file_system_policy)\napplication_id = \"APPLICATION_ID\"\njob = dsf.SparkEmrServerlessJob(self, \"SparkJob\",\n    job_config={\n        \"Name\": JsonPath.format(\"ge_profile-{}\", JsonPath.uuid()),\n        \"ApplicationId\": application_id,\n        \"ExecutionRoleArn\": my_execution_role.role_arn,\n        \"JobDriver\": {\n            \"SparkSubmit\": {\n                \"EntryPoint\": \"s3://S3-BUCKET/pi.py\",\n                \"EntryPointArguments\": [],\n                \"SparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\"\n            }\n        }\n    }\n)\n\ncdk.CfnOutput(self, \"SparkJobStateMachine\",\n    value=job.state_machine.state_machine_arn\n)","version":"2"},"csharp":{"source":"using Amazon.CDK.AWS.IAM;\nusing Amazon.CDK.AWS.StepFunctions;\n\n\nvar myFileSystemPolicy = new PolicyDocument(new PolicyDocumentProps {\n    Statements = new [] { new PolicyStatement(new PolicyStatementProps {\n        Actions = new [] { \"s3:GetObject\" },\n        Resources = new [] { \"*\" }\n    }) }\n});\n\nvar myExecutionRole = SparkEmrServerlessRuntime.CreateExecutionRole(this, \"execRole1\", myFileSystemPolicy);\nvar applicationId = \"APPLICATION_ID\";\nvar job = new SparkEmrServerlessJob(this, \"SparkJob\", (SparkEmrServerlessJobApiProps)new SparkEmrServerlessJobApiProps {\n    JobConfig = new Dictionary<string, object> {\n        { \"Name\", JsonPath.Format(\"ge_profile-{}\", JsonPath.Uuid()) },\n        { \"ApplicationId\", applicationId },\n        { \"ExecutionRoleArn\", myExecutionRole.RoleArn },\n        { \"JobDriver\", new Dictionary<string, IDictionary<string, object>> {\n            { \"SparkSubmit\", new Struct {\n                EntryPoint = \"s3://S3-BUCKET/pi.py\",\n                EntryPointArguments = new [] {  },\n                SparkSubmitParameters = \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\"\n            } }\n        } }\n    }\n});\n\nnew CfnOutput(this, \"SparkJobStateMachine\", new CfnOutputProps {\n    Value = job.StateMachine.StateMachineArn\n});","version":"1"},"java":{"source":"import software.amazon.awscdk.services.iam.PolicyDocument;\nimport software.amazon.awscdk.services.iam.PolicyStatement;\nimport software.amazon.awscdk.services.stepfunctions.JsonPath;\n\n\nPolicyDocument myFileSystemPolicy = PolicyDocument.Builder.create()\n        .statements(List.of(PolicyStatement.Builder.create()\n                .actions(List.of(\"s3:GetObject\"))\n                .resources(List.of(\"*\"))\n                .build()))\n        .build();\n\nRole myExecutionRole = SparkEmrServerlessRuntime.createExecutionRole(this, \"execRole1\", myFileSystemPolicy);\nString applicationId = \"APPLICATION_ID\";\nSparkEmrServerlessJob job = new SparkEmrServerlessJob(this, \"SparkJob\", (SparkEmrServerlessJobApiProps)SparkEmrServerlessJobApiProps.builder()\n        .jobConfig(Map.of(\n                \"Name\", JsonPath.format(\"ge_profile-{}\", JsonPath.uuid()),\n                \"ApplicationId\", applicationId,\n                \"ExecutionRoleArn\", myExecutionRole.getRoleArn(),\n                \"JobDriver\", Map.of(\n                        \"SparkSubmit\", Map.of(\n                                \"EntryPoint\", \"s3://S3-BUCKET/pi.py\",\n                                \"EntryPointArguments\", List.of(),\n                                \"SparkSubmitParameters\", \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\"))))\n        .build());\n\nCfnOutput.Builder.create(this, \"SparkJobStateMachine\")\n        .value(job.getStateMachine().getStateMachineArn())\n        .build();","version":"1"},"go":{"source":"import \"github.com/aws/aws-cdk-go/awscdk\"\nimport \"github.com/aws/aws-cdk-go/awscdk\"\n\n\nmyFileSystemPolicy := awscdk.NewPolicyDocument(&PolicyDocumentProps{\n\tStatements: []policyStatement{\n\t\tawscdk.NewPolicyStatement(&PolicyStatementProps{\n\t\t\tActions: []*string{\n\t\t\t\tjsii.String(\"s3:GetObject\"),\n\t\t\t},\n\t\t\tResources: []*string{\n\t\t\t\tjsii.String(\"*\"),\n\t\t\t},\n\t\t}),\n\t},\n})\n\nmyExecutionRole := dsf.SparkEmrServerlessRuntime_CreateExecutionRole(this, jsii.String(\"execRole1\"), myFileSystemPolicy)\napplicationId := \"APPLICATION_ID\"\njob := dsf.NewSparkEmrServerlessJob(this, jsii.String(\"SparkJob\"), &SparkEmrServerlessJobApiProps{\n\tJobConfig: map[string]interface{}{\n\t\t\"Name\": awscdk.JsonPath_format(jsii.String(\"ge_profile-{}\"), awscdk.JsonPath_uuid()),\n\t\t\"ApplicationId\": applicationId,\n\t\t\"ExecutionRoleArn\": myExecutionRole.roleArn,\n\t\t\"JobDriver\": map[string]map[string]interface{}{\n\t\t\t\"SparkSubmit\": map[string]interface{}{\n\t\t\t\t\"EntryPoint\": jsii.String(\"s3://S3-BUCKET/pi.py\"),\n\t\t\t\t\"EntryPointArguments\": []interface{}{\n\t\t\t\t},\n\t\t\t\t\"SparkSubmitParameters\": jsii.String(\"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\"),\n\t\t\t},\n\t\t},\n\t},\n}.(sparkEmrServerlessJobApiProps))\n\ncdk.NewCfnOutput(this, jsii.String(\"SparkJobStateMachine\"), &CfnOutputProps{\n\tValue: job.StateMachine.StateMachineArn,\n})","version":"1"},"$":{"source":"import { PolicyDocument, PolicyStatement } from 'aws-cdk-lib/aws-iam';\nimport { JsonPath } from 'aws-cdk-lib/aws-stepfunctions';\n\nconst myFileSystemPolicy = new PolicyDocument({\n  statements: [new PolicyStatement({\n    actions: [\n      's3:GetObject',\n    ],\n    resources: ['*'],\n  })],\n});\n\n\nconst myExecutionRole = dsf.SparkEmrServerlessRuntime.createExecutionRole(this, 'execRole1', myFileSystemPolicy);\nconst applicationId = \"APPLICATION_ID\";\nconst job = new dsf.SparkEmrServerlessJob(this, 'SparkJob', {\n  jobConfig:{\n    \"Name\": JsonPath.format('ge_profile-{}', JsonPath.uuid()),\n    \"ApplicationId\": applicationId,\n    \"ExecutionRoleArn\": myExecutionRole.roleArn,\n    \"JobDriver\": {\n      \"SparkSubmit\": {\n          \"EntryPoint\": \"s3://S3-BUCKET/pi.py\",\n          \"EntryPointArguments\": [],\n          \"SparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\"\n      },\n    }\n  }\n} as dsf.SparkEmrServerlessJobApiProps);\n\nnew cdk.CfnOutput(this, 'SparkJobStateMachine', {\n  value: job.stateMachine!.stateMachineArn,\n});","version":"0"}},"location":{"api":{"api":"type","fqn":"aws-dsf.SparkEmrServerlessJob"},"field":{"field":"example"}},"didCompile":true,"fqnsReferenced":["aws-cdk-lib.CfnOutput","aws-cdk-lib.CfnOutputProps","aws-cdk-lib.aws_iam.PolicyDocument","aws-cdk-lib.aws_iam.PolicyDocumentProps","aws-cdk-lib.aws_iam.PolicyStatement","aws-cdk-lib.aws_iam.PolicyStatementProps","aws-cdk-lib.aws_iam.Role","aws-cdk-lib.aws_iam.Role#roleArn","aws-cdk-lib.aws_stepfunctions.JsonPath#format","aws-cdk-lib.aws_stepfunctions.JsonPath#uuid","aws-cdk-lib.aws_stepfunctions.StateMachine#stateMachineArn","aws-dsf.SparkEmrServerlessJob","aws-dsf.SparkEmrServerlessRuntime","aws-dsf.SparkEmrServerlessRuntime#createExecutionRole","aws-dsf.SparkJob#stateMachine","constructs.Construct"],"fullSource":"// Hoisted imports begin after !show marker below\n/// !show\nimport { PolicyDocument, PolicyStatement } from 'aws-cdk-lib/aws-iam';\nimport { JsonPath } from 'aws-cdk-lib/aws-stepfunctions';\n/// !hide\n// Hoisted imports ended before !hide marker above\nimport { Construct } from 'constructs';\nimport * as cdk from 'aws-cdk-lib';\nimport * as dsf from 'aws-dsf';\n\nclass MyStack extends cdk.Stack {\n  constructor(scope: Construct, id: string) {\n    super(scope, id);\n\n    // Code snippet begins after !show marker below\n/// !show\n\n\nconst myFileSystemPolicy = new PolicyDocument({\n  statements: [new PolicyStatement({\n    actions: [\n      's3:GetObject',\n    ],\n    resources: ['*'],\n  })],\n});\n\n\nconst myExecutionRole = dsf.SparkEmrServerlessRuntime.createExecutionRole(this, 'execRole1', myFileSystemPolicy);\nconst applicationId = \"APPLICATION_ID\";\nconst job = new dsf.SparkEmrServerlessJob(this, 'SparkJob', {\n  jobConfig:{\n    \"Name\": JsonPath.format('ge_profile-{}', JsonPath.uuid()),\n    \"ApplicationId\": applicationId,\n    \"ExecutionRoleArn\": myExecutionRole.roleArn,\n    \"JobDriver\": {\n      \"SparkSubmit\": {\n          \"EntryPoint\": \"s3://S3-BUCKET/pi.py\",\n          \"EntryPointArguments\": [],\n          \"SparkSubmitParameters\": \"--conf spark.executor.instances=2 --conf spark.executor.memory=2G --conf spark.driver.memory=2G --conf spark.executor.cores=4\"\n      },\n    }\n  }\n} as dsf.SparkEmrServerlessJobApiProps);\n\nnew cdk.CfnOutput(this, 'SparkJobStateMachine', {\n  value: job.stateMachine!.stateMachineArn,\n});\n/// !hide\n// Code snippet ended before !hide marker above\n  }\n}","syntaxKindCounter":{"11":19,"80":34,"110":3,"166":1,"183":1,"209":4,"210":7,"211":9,"213":3,"214":4,"234":1,"235":1,"243":4,"244":1,"260":4,"261":4,"272":2,"273":2,"275":2,"276":3,"303":13,"312":1},"fqnsFingerprint":"9921088ccf2b4c82bcbe156212e6ba11f4b735bf0ec46103b513d248eb7f4d72"}}}